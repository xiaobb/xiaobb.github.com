---
date: 2012-12-20 21:00:00
layout: post
title: MapReduce之dumbo：Hadoop的Python编程接口
categories:
    - Tech
tags: 
    - MapReduce
    - OpenSource
    - Python
    - Dumbo
comments: yes
---

ms实习回来后，一直在捣鼓MapReduce,搭建好Hadoop平台（27+节点，关灯后看网卡灯闪烁还是挺壮观的，呵呵）后，就开始找基于Python的编程接口。
目前有几个主流的Python接口：

 * [Happy](https://code.google.com/p/happy/) 基于Jython的，原理是将Python代码转化为Java代码，不推荐使用，因为Jython严重限制了Python库的使用。
 * [Pydoop](http://pydoop.sourceforge.net/) 基于Cython实现，利用C语言底层的pipeline机制，可以使用Numpy之类的C语言库，而且能在C语言层面对HDFS进行操作，同时
  能够对MR compnents(RecordReader, RecordWriter, Patitioner)进行访问，最后还能访问Hadoop的配置文件，设置计数器等，看起来很不错。
 * [Hadoopy](http://www.hadoopy.com/en/latest/) 又一个基于Cython实现的Python编程接口，代码非常简练，文档也完善
 * [dumbo](https://github.com/klbostee/dumbo/wiki) 是纯Python实现的框架，利用Hadoop的Streamming机制，最后选择的也是这个框架，原因只有一点，足够Pythonic，让人爱不释手。
 * [Python native](http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/) 也可以用Python自己写STDIN和STDOUT来做，链接里的是一篇非常好的介绍。

总结一下在MapReduce编程和dumbo使用中遇到的一些问题和解决方法。

------

## 调试方法

  * stderr: 由于dumbo是利用python的标准输入和输出流与hadoop进行通讯，所以绝对不能使用print直接打印变量。可以使用sys.stderr向错误流中输出，
  这样在运行中就可以直接观察运行结果。
  实例：
    {% highlight python %}
        def debug(content,pos=None):
            print >> sys.stderr, "+"*15
            if pos is not None:print >> sys.stderr, pos
            print >> sys.stderr, content
            print >> sys.stderr, "-"*15 {% endhighlight %}
  
  * dumbo 调试变量：当使用Mapper和Reducer类时，除了可以方便的进行初始化外，还可以携带一些变量，监控执行过程，并且能在Hadoop WEB页面展示出来。
     * params: 一个字典结构，可以携带外部变量，用来传递一些变量，文件等。
     * counters: 可以视为一个defaultdict，包含dumbo.Counter objects。
     * status: 该字段可以将状态或调试信息展示在Hadoop的web interface。

  * remote debugger: 在dumbo中嵌入远程调试器，如Winpdb或Komodo IDE。参看[这里](https://groups.google.com/forum/?fromgroups=#!topic/dumbo-user/SLN3teTKgIU)，
  还有[这里](https://gist.github.com/866301)。

to be continued...

